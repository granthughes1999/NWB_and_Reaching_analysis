{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file is to take the .xlsx file created by reach_Curator_py38_v3.py script, and adds in the missing rows for T6000, T5000 values\n",
    "- has multiple steps\n",
    "1. Backs up the raw .xlsx file created by Reach_Curator_py38_v3.py\n",
    "2. Takes raw .xlsx file created by Reach_Curator_py38_v3.py --> Adds in missing rows by reading _events.txt file and comparing it --> puts this modifed version with added rows in the root folder Grant_curate\n",
    "3. Once you have manually curated the session (ie. added reaches) using the Reach_Curator_py38_v3.py\n",
    "4. this will back up that manually curated .xlsx file into this folder, which it creates if it does not exist already --> completed_manual_curation_backup\n",
    "5. finally, it will remove any rows that have no values for ReachInit > ReachMax > ReachEnd_ --> it will then put this final cleaned version back into the root folder that the Reach_Curator_py38_v3.py saves and reads from --> Grant_curate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⭐ What this file does ⭐\n",
    "\n",
    "## ✅ Results of running this file\n",
    "1. you have created a duplicate of the .xlsx file that the reach_curator_py38_v3.py makes when you load in a session for the first time \n",
    "    - Backup location --> Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/reach_curator_direct_backup\n",
    "    - Note, this file is missing all the T6000 and T5000 rows that the find_reach_events.ipynb did not associate a reach too\n",
    "2. you have created a new .xlsx file that now containes every sinlge row, meaning it added rows for every single T6000 > T5000, and left the reachInit > reachMax > reachEnd vlaues empty, this way you can manually go into the curator and add reaches for these rows\n",
    "    - Backup location --> Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/added_missing_rows_backup\n",
    "\n",
    "    \n",
    "### ✅ Next Steps after running this file\n",
    "1. Go back to reach_Curator_p38_v3.py, and simply launch it liuke normal. then manually place all the reaches you need \n",
    "\n",
    "2. Once 100% done adding reachs and label. open the analyze_curated_reach_results.ipynb file (its in the same folder as this notebook)\n",
    "    - then simply run that file\n",
    "    - it will first back up your manaual curation .xlsx file too --> Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/completed_manual_curation_backup\n",
    "    - it will extract / drop all empty rows from that .xlsx file and save the final version too -->  Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/final_backup\n",
    "    - and it will save this final_df to the root folder, so the reach_curator can load it in --> Grant_curate/final_xlsx_file.xlsx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify these three variables as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOUSE loaded: reach3\n",
      "\n",
      "\n",
      "-- First Neuropixels File --\n",
      "NP_FILE loaded: reach3_01_2024-11-14_21-30-18_001\n",
      "DATE loaded: 20241114\n",
      "SESSION loaded: session010\n",
      "\n",
      "\n",
      "-- Second Neuropixels File --\n",
      "NP_FILE loaded: reach3_02_2024-11-14_23-57-24_002\n",
      "DATE_01 loaded: 20241114\n",
      "SESSION_01 loaded: session011\n",
      "\n",
      "\n",
      "-- Third Neuropixels File --\n",
      "NP_FILE_02 loaded: NA\n",
      "DATE_02 loaded: NA\n",
      "SESSION_02 loaded: NA\n",
      "\n",
      "\n",
      "-- Behavioral Files --\n",
      "BEHAVIORAL_FOLDER loaded: grant_reach3_swingDoor-christie\n",
      "Neuropixel Recording folder found: G:\\Grant\\neuropixels\\kilosort_recordings\\reach3_01_2024-11-14_21-30-18_001\\Record Node 103\\experiment1\\recording1\\continuous\n"
     ]
    }
   ],
   "source": [
    "# Load mouse_name from .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure that the MOUSE_NAME environment variable is set\n",
    "MOUSE = os.getenv('MOUSE')\n",
    "if MOUSE is None:\n",
    "    raise ValueError(\"MOUSE_NAME environment variable is not set. Please set it to the name of the mouse.\")\n",
    "else:\n",
    "    print(f\"MOUSE loaded: {MOUSE}\")\n",
    "    print('\\n')\n",
    "\n",
    "print('-- First Neuropixels File --')\n",
    "NP_FILE = os.getenv('NP_FILE')\n",
    "if NP_FILE is None:\n",
    "    raise ValueError(\"NP_FILE environment variable is not set. Please set it to the path of the neuropixels file.\")\n",
    "else:\n",
    "    print(f\"NP_FILE loaded: {NP_FILE}\")\n",
    "\n",
    "DATE = os.getenv('DATE')\n",
    "if DATE is None:\n",
    "    raise ValueError(\"DATE environment variable is not set. Please set it to the date of the recording.\")\n",
    "else:\n",
    "    print(f\"DATE loaded: {DATE}\")\n",
    "\n",
    "SESSION = os.getenv('SESSION')\n",
    "if SESSION is None:\n",
    "    raise ValueError(\"SESSION environment variable is not set. Please set it to the session number.\")\n",
    "else:\n",
    "    print(f\"SESSION loaded: {SESSION}\")\n",
    "\n",
    "print('\\n')\n",
    "print('-- Second Neuropixels File --')\n",
    "NP_FILE_01 = os.getenv('NP_FILE_01')\n",
    "if NP_FILE_01 is None:\n",
    "    raise ValueError(\"NP_FILE_01 environment variable is not set. Please set it to the path of the neuropixels file.\")\n",
    "else:\n",
    "    print(f\"NP_FILE loaded: {NP_FILE_01}\")\n",
    "\n",
    "DATE_01 = os.getenv('DATE_01')\n",
    "if DATE_01 is None:\n",
    "    raise ValueError(\"DATE_01 environment variable is not set. Please set it to the date of the recording in MMDD format.\")\n",
    "else:\n",
    "    print(f\"DATE_01 loaded: {DATE_01}\")\n",
    "\n",
    "SESSION_01 = os.getenv('SESSION_01')\n",
    "if SESSION_01 is None:\n",
    "    raise ValueError(\"SESSION_01 environment variable is not set. Please set it to the session number in MMDD format.\")\n",
    "else:\n",
    "    print(f\"SESSION_01 loaded: {SESSION_01}\")\n",
    "\n",
    "print('\\n')\n",
    "print('-- Third Neuropixels File --')\n",
    "NP_FILE_02 = os.getenv('NP_FILE_02')\n",
    "if NP_FILE_02 is None:\n",
    "    raise ValueError(\"NP_FILE_02 environment variable is not set. Please set it to the path of the second neuropixels file.\")\n",
    "else:\n",
    "    print(f\"NP_FILE_02 loaded: {NP_FILE_02}\")\n",
    "\n",
    "DATE_02 = os.getenv('DATE_02')\n",
    "if DATE_02 is None:\n",
    "    raise ValueError(\"DATE_02 environment variable is not set. Please set it to the date of the recording in MMDD format.\")\n",
    "else:\n",
    "    print(f\"DATE_02 loaded: {DATE_02}\")\n",
    "\n",
    "SESSION_02 = os.getenv('SESSION_02')\n",
    "if SESSION_02 is None:\n",
    "    raise ValueError(\"SESSION_02 environment variable is not set. Please set it to the session number in MMDD format.\")\n",
    "else:\n",
    "    print(f\"SESSION_02 loaded: {SESSION_02}\")\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('-- Behavioral Files --')\n",
    "BEHAVIORAL_FOLDER = os.getenv('BEHAVIORAL_FOLDER')\n",
    "if BEHAVIORAL_FOLDER is None:\n",
    "    raise ValueError(\"BEHAVIORAL_FOLDER environment variable is not set. Please set it to the path of the behavioral recordings.\")\n",
    "else:\n",
    "    print(f\"BEHAVIORAL_FOLDER loaded: {BEHAVIORAL_FOLDER}\")\n",
    "\n",
    "# Construct the recording folder path\n",
    "root_recording_folder = fr\"G:\\Grant\\neuropixels\\kilosort_recordings\\{NP_FILE}\"  # Replace with actual root path\n",
    "recording_folder = os.path.join(root_recording_folder, \"Record Node 103\", \"experiment1\", \"recording1\", \"continuous\")\n",
    "\n",
    "#root_behavior_path = fr\"G:\\Grant\\neuropixels\\behavioral_recordings\\{BEHAVIORAL_FOLDER}\"\n",
    "# Example values (replace as needed)A\n",
    "root_behavior_path = fr\"G:\\Grant\\behavior_data\\DLC_net\\{BEHAVIORAL_FOLDER}\"\n",
    "behavioral_folder = os.path.join(root_behavior_path, DATE, SESSION)\n",
    "\n",
    "# Check if the recording folder exists\n",
    "if not os.path.exists(recording_folder):\n",
    "    raise FileNotFoundError(f\"Recording folder not found: {recording_folder}\")\n",
    "else:\n",
    "    print(\"Neuropixel Recording folder found:\", recording_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in intermeidate files \n",
    "- df_closedLoop_stim_loaded --> this file has all the closed loop optical time stamps aligned to each trial, providing both the Neuropixel timestamp And the actual associated frame number  \n",
    "1. \"T5000\", \"closed_loop_stim\" --> are the Neuropixel Timestamps \n",
    "2. \"T5000_frame\" --> is the associated frame number \n",
    "3. \"stim_count\" --> is the optical stim number, should go up by 10 each row, assuming you did 10 optical pulses per trail\n",
    "4. \"stim_present\" --> is self-explanitory, True if there was a stim for that trial, NaN if there was not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: if this cell fails to load in the DF, you can assume it did not save\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T5000</th>\n",
       "      <th>closed_loop_stim</th>\n",
       "      <th>stim_count</th>\n",
       "      <th>stim_present</th>\n",
       "      <th>T5000_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2188.344875</td>\n",
       "      <td>2188.856325</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2202.343700</td>\n",
       "      <td>2203.343900</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>4002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2216.854500</td>\n",
       "      <td>2217.345475</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>6102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2230.595325</td>\n",
       "      <td>2231.092400</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>8164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2246.842125</td>\n",
       "      <td>2247.347050</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>10601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         T5000  closed_loop_stim  stim_count stim_present  T5000_frame\n",
       "0  2188.344875       2188.856325           0         True         1829\n",
       "1  2202.343700       2203.343900          10         True         4002\n",
       "2  2216.854500       2217.345475          20         True         6102\n",
       "3  2230.595325       2231.092400          30         True         8164\n",
       "4  2246.842125       2247.347050          40         True        10601"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f'Note: if this cell fails to load in the DF, you can assume it did not save')\n",
    "int_dir   = Path(root_recording_folder) / 'intermediates'\n",
    "file_path = int_dir / 'T5000_closedLoop_stim_df.csv'\n",
    "\n",
    "# now load:\n",
    "df_closedLoop_stim_loaded = pd.read_csv(file_path)\n",
    "df_closedLoop_stim_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20241114_session010\n"
     ]
    }
   ],
   "source": [
    "root_path = r'G:\\Grant\\behavior_data\\DLC_net' # change this to your root path where all the analysis folder live, i currently use is G:\\Grant\\behavior_data\\DLC_net\n",
    "\n",
    "root_folder = BEHAVIORAL_FOLDER # change this to your root folder for one specfic mouse (this should contain multiple sessions)\n",
    "\n",
    "\n",
    "SESSION_OPTIONS = [f\"{DATE}_{SESSION}\", f\"{DATE_01}_{SESSION_01}\", f\"{DATE_02}_{SESSION_02}\"]\n",
    "# Set the specific mouse session to analyze\n",
    "\n",
    "session_info = SESSION_OPTIONS[0] # change this to the session you want to analyze\n",
    "print(session_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new session_option for each session\n",
    "- class that lets you load data for specific mouse sesssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_path: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\n",
      "mouse_info: {'mouse': 'reach3', 'session_date': '20241114', 'session_ID': 'session010'}\n",
      "xlsx_file: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\20241114_christielab_session010.xlsx\n"
     ]
    }
   ],
   "source": [
    "main_path = rf'{root_path}\\{root_folder}'\n",
    "print(f'main_path: {main_path}')\n",
    "\n",
    "class select_mouse_session:\n",
    "    def __init__(self, mouse, session_date, session_ID):\n",
    "        self.mouse = mouse\n",
    "        self.session_date = session_date\n",
    "        self.session_ID = session_ID\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Mouse: {self.mouse}, Session Date: {self.session_date}, Session ID: {self.session_ID}\"\n",
    "    \n",
    "    def get_mouse_session_files(self):\n",
    "        mouse_session = f\"{self.mouse}_{self.session_date}_{self.session_ID}\"\n",
    "        xlsx_file = rf'{main_path}\\Grant_curate\\{self.session_date}_christielab_{self.session_ID}.xlsx'\n",
    "        txt_file = rf\"{main_path}\\videos\\{self.session_date}\\christielab\\{self.session_ID}\\{self.session_date}_christielab_{self.session_ID}_events_shifted.txt\"\n",
    "        xlsx_orig = rf'{main_path}\\Grant_curate\\xlsx_backups\\curator_direct_backup\\{self.session_date}_christielab_{self.session_ID}.xlsx'\n",
    "        \n",
    "        return xlsx_file, txt_file, xlsx_orig\n",
    "    \n",
    "    def get_mouse_info(self):\n",
    "        mouse_info = {\n",
    "            'mouse': self.mouse,\n",
    "            'session_date': self.session_date,\n",
    "            'session_ID': self.session_ID\n",
    "        }\n",
    "        return mouse_info\n",
    "    \n",
    "\n",
    "# Strip the session info to get the session date and ID\n",
    "session_date = session_info.split('_')[0]\n",
    "session_ID = session_info.split('_')[1]\n",
    "\n",
    "# Strip the mouse name from the root folder\n",
    "mouse = root_folder.split('_')[1]\n",
    "\n",
    "# Create an instance of the class with the mouse name, session date, and session ID\n",
    "class_intsance = select_mouse_session(mouse=mouse, session_date=session_date, session_ID=session_ID)\n",
    "\n",
    "# Get the mouse session files and info\n",
    "xlsx_file, txt_file, xlsx_orig = class_intsance.get_mouse_session_files()\n",
    "mouse_info = class_intsance.get_mouse_info()\n",
    "\n",
    "# Print the results\n",
    "print(f'mouse_info: {mouse_info}')\n",
    "print(f'xlsx_file: {xlsx_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "session_save_name: reach3_20241114_session010\n",
      "\n",
      "mouse: reach3\n",
      "session_date: 20241114\n",
      "session_ID: session010\n",
      "original_dir: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\n",
      "backup_dir: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\reach3_20241114_session010\\reach_curator_direct_backup\n"
     ]
    }
   ],
   "source": [
    "session_date = mouse_info['session_date']\n",
    "session_ID = mouse_info['session_ID']\n",
    "mouse = mouse_info['mouse']\n",
    "session_save_name = f\"{mouse}_{session_date}_{session_ID}\"\n",
    "print('=========================')\n",
    "print(f'session_save_name: {session_save_name}')\n",
    "print('')\n",
    "print(f'mouse: {mouse}')\n",
    "print(f'session_date: {session_date}')\n",
    "print(f'session_ID: {session_ID}')\n",
    "\n",
    "\n",
    "    # ---- Step 1: Make a backup in curator_backup folder\n",
    "original_dir = os.path.dirname(xlsx_file)\n",
    "backup_dir = os.path.join(original_dir, 'xlsx_backups',session_save_name,'reach_curator_direct_backup')\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "print(f'original_dir: {original_dir}')\n",
    "print(f'backup_dir: {backup_dir}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if you have already analyzed this session\n",
    "- if you have, the code is going to stop you from re-analyzing it by KILLING the keneral\n",
    "- you you do want to overwrite these files --> you need to re-run from the top and skip this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Looks like you have not completed the first step for this session yet ✅\n",
      "⭐=============== Therefore you should continue running this file =======================⭐\n"
     ]
    }
   ],
   "source": [
    "final_df_backup = os.path.join(os.path.dirname(xlsx_file), 'xlsx_backups',session_save_name ,'final_backup')\n",
    "final_df_backup_file_path = os.path.join(final_df_backup, os.path.basename(xlsx_file))\n",
    "\n",
    "manual_curation_backup = os.path.join(os.path.dirname(xlsx_file),'xlsx_backups', session_save_name,'completed_manual_curation_backup')\n",
    "manual_curation_backup = os.path.join(manual_curation_backup, os.path.basename(xlsx_file))\n",
    "\n",
    "if os.path.exists(manual_curation_backup):\n",
    "    print('⚠️ Looks like you have already completed the manual curation for this session ⚠️')\n",
    "    print('❌ Therefore, in an effort to prevent overwriting any files, the script changed is going to KILL the kernal (sorry)❌')\n",
    "    print('')\n",
    "    print(f'manual_curation_backup: {manual_curation_backup}')\n",
    "    print('')\n",
    "    print('===========================================')\n",
    "    print(' ✅ if you actually want to run this script on this session then simply skip running this cell next time ✅')\n",
    "    print('===========================================')\n",
    "    exit()\n",
    "\n",
    "print('')\n",
    "if os.path.exists(final_df_backup_file_path):\n",
    "    print('⚠️ Looks like you have already completed the final curation for this session ⚠️')\n",
    "    print('❌ Therefore, in an effort to prevent overwriting any files, the script changed is going to KILL the kernal (sorry)❌')\n",
    "    print('')\n",
    "    print(f'final_df_backup_file_path: {final_df_backup_file_path}')\n",
    "    print('')\n",
    "    print('===========================================')\n",
    "    print(' ✅ if you actually want to run this script on this session then simply skip running this cell next time ✅')\n",
    "    print('===========================================')    \n",
    "    exit()\n",
    "\n",
    "print('✅ Looks like you have not completed the first step for this session yet ✅')\n",
    "print('⭐=============== Therefore you should continue running this file =======================⭐')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify existence of session files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️Warning: Backup file already exists at G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\reach3_20241114_session010\\reach_curator_direct_backup\\20241114_christielab_session010.xlsx⚠️\n",
      "backup_dir: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\reach3_20241114_session010\\reach_curator_direct_backup\\20241114_christielab_session010.xlsx\n",
      "=========================\n",
      "xlsx_file: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\20241114_christielab_session010.xlsx\n",
      "=========================\n",
      "txt_file: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\videos\\20241114\\christielab\\session010\\20241114_christielab_session010_events_shifted.txt\n",
      "=========================\n",
      "xlsx_orig: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\curator_direct_backup\\20241114_christielab_session010.xlsx\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(txt_file):\n",
    "    print(f\"⚠️Warning: {txt_file} does not exist⚠️\")\n",
    "    print('--> You need to run this behavioral session with the newest version of find_reach_events.py first to create the txt file')\n",
    "    print('\\n')\n",
    "\n",
    "backup_file_path = os.path.join(backup_dir, os.path.basename(xlsx_file))\n",
    "if os.path.exists(backup_file_path):\n",
    "    print(f\"⚠️Warning: Backup file already exists at {backup_file_path}⚠️\")\n",
    "else:\n",
    "        # Check if files exist\n",
    "    if not os.path.exists(xlsx_file):\n",
    "        print(f\"⚠️Warning: {xlsx_file} does not exist⚠️\")\n",
    "        print('--> You need to open this behavioral session with reach_curator_v3.py first to create the xlsx file')\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Copy the original file to the backup directory\n",
    "        shutil.copy2(xlsx_file, backup_file_path)\n",
    "        print(f\"✅ Backup saved to: {backup_file_path}\")\n",
    "\n",
    "# ---- (your parsing & merging code goes here)\n",
    "# ... your code to build `combined` DataFrame ...\n",
    "\n",
    "print(f'backup_dir: {backup_file_path}')\n",
    "print('=========================')\n",
    "print(f'xlsx_file: {xlsx_file}')\n",
    "print('=========================')\n",
    "print(f'txt_file: {txt_file}')\n",
    "print('=========================')\n",
    "print(f'xlsx_orig: {xlsx_orig}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the cell that takes the .xlsx file and the .txt file and creates a new .xlsx file with the manual additions\n",
    "- makes a new folder called manual_additions if it does not exist\n",
    "- reads the .xlsx file into a pandas dataframe\n",
    "- reads the .txt file into a pandas dataframe\n",
    "- creates a new dataframe with the manual additions\n",
    "- saves the new dataframe to a new .xlsx file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing xlsx file: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\20241114_christielab_session010.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T6000</th>\n",
       "      <th>T5000</th>\n",
       "      <th>Added_row</th>\n",
       "      <th>reachInit</th>\n",
       "      <th>reachMax</th>\n",
       "      <th>reachEnd</th>\n",
       "      <th>stim</th>\n",
       "      <th>behaviors</th>\n",
       "      <th>pellet_delivery</th>\n",
       "      <th>pellet_detected</th>\n",
       "      <th>ReachType</th>\n",
       "      <th>pellet_present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1744</td>\n",
       "      <td>1829</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1882.0</td>\n",
       "      <td>1895.0</td>\n",
       "      <td>0</td>\n",
       "      <td>missed</td>\n",
       "      <td>1826</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>short</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3843</td>\n",
       "      <td>4002</td>\n",
       "      <td>4002.0</td>\n",
       "      <td>4058.0</td>\n",
       "      <td>4075.0</td>\n",
       "      <td>4106.0</td>\n",
       "      <td>0</td>\n",
       "      <td>dropped</td>\n",
       "      <td>3999</td>\n",
       "      <td>4076.0</td>\n",
       "      <td>short</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6019</td>\n",
       "      <td>6103</td>\n",
       "      <td>6103.0</td>\n",
       "      <td>6143.0</td>\n",
       "      <td>6158.0</td>\n",
       "      <td>6731.0</td>\n",
       "      <td>0</td>\n",
       "      <td>dropped</td>\n",
       "      <td>6100</td>\n",
       "      <td>6177.0</td>\n",
       "      <td>on_pellet</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8081</td>\n",
       "      <td>8165</td>\n",
       "      <td>8165.0</td>\n",
       "      <td>8210.0</td>\n",
       "      <td>8223.0</td>\n",
       "      <td>8235.0</td>\n",
       "      <td>0</td>\n",
       "      <td>stalled</td>\n",
       "      <td>8162</td>\n",
       "      <td>8237.0</td>\n",
       "      <td>short</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8081</td>\n",
       "      <td>8165</td>\n",
       "      <td>8165.0</td>\n",
       "      <td>8362.0</td>\n",
       "      <td>8373.0</td>\n",
       "      <td>10406.0</td>\n",
       "      <td>0</td>\n",
       "      <td>dropped</td>\n",
       "      <td>8162</td>\n",
       "      <td>8237.0</td>\n",
       "      <td>on_pellet</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   T6000  T5000  Added_row  reachInit  reachMax  reachEnd  stim behaviors  \\\n",
       "0   1744   1829     1829.0     1868.0    1882.0    1895.0     0    missed   \n",
       "1   3843   4002     4002.0     4058.0    4075.0    4106.0     0   dropped   \n",
       "2   6019   6103     6103.0     6143.0    6158.0    6731.0     0   dropped   \n",
       "3   8081   8165     8165.0     8210.0    8223.0    8235.0     0   stalled   \n",
       "4   8081   8165     8165.0     8362.0    8373.0   10406.0     0   dropped   \n",
       "\n",
       "   pellet_delivery  pellet_detected  ReachType  pellet_present  \n",
       "0             1826           1900.0      short             1.0  \n",
       "1             3999           4076.0      short             1.0  \n",
       "2             6100           6177.0  on_pellet             1.0  \n",
       "3             8162           8237.0      short             1.0  \n",
       "4             8162           8237.0  on_pellet             1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load existing curated xlsx\n",
    "print(f\"Loading existing xlsx file: {xlsx_file}\")\n",
    "df_orig = pd.read_excel(xlsx_file)\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class modify_curator_xlsx_file(object):\n",
    "    def __init__(self, xlsx_file, txt_file,save_df=True):\n",
    "        self.txt_file = txt_file\n",
    "        self.xlsx_file = xlsx_file\n",
    "        self.save_df = save_df\n",
    "\n",
    "        \n",
    "    def create_new_xlxs_file_v1(self):\n",
    "        save_df = self.save_df\n",
    "        # Load the existing xlsx file\n",
    "        df_orig = pd.read_excel(self.xlsx_file)\n",
    "        # Check if the file is empty\n",
    "        txt_file = self.txt_file\n",
    "        if not os.path.exists(txt_file):\n",
    "            print(f\"⚠️Warning: {txt_file} does not exist⚠️\")\n",
    "            return None\n",
    "        # Check if the xlsx file is empty\n",
    "        xlsx_file = self.xlsx_file\n",
    "        if not os.path.exists(xlsx_file):\n",
    "            print(f\"⚠️Warning: {xlsx_file} does not exist⚠️\")\n",
    "            return None\n",
    "\n",
    "        if df_orig.empty:\n",
    "            print(\"⚠️Warning: The xlsx file is empty⚠️\")\n",
    "            return None\n",
    "        \n",
    "        # Normalize column names just in case\n",
    "        df_orig.columns = df_orig.columns.str.strip()\n",
    "\n",
    "        # Load and parse txt as done earlier\n",
    "        events = []\n",
    "        current_event = {}\n",
    "        with open(txt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    key, value = line.strip().split('\\t')\n",
    "                    value = int(value)\n",
    "                    if key == 'T6000_played':\n",
    "                        if current_event:\n",
    "                            events.append(current_event)\n",
    "                        current_event = {'T6000': value}\n",
    "                    elif key == 'pellet_detected':\n",
    "                        current_event['pellet_detected'] = value\n",
    "                    elif key == 'pellet_delivery':\n",
    "                        current_event['pellet_delivery'] = value\n",
    "                    elif key == 'T5000_played':\n",
    "                        current_event['T5000'] = value\n",
    "        if current_event:\n",
    "            events.append(current_event)\n",
    "\n",
    "        events_df = pd.DataFrame(events)\n",
    "\n",
    "        # Find which T6000s are missing from original file\n",
    "        existing_T6000s = df_orig['T6000'].values\n",
    "        new_rows = events_df[~events_df['T6000'].isin(existing_T6000s)].copy()\n",
    "\n",
    "        # Fill in missing columns to match structure\n",
    "        new_rows['reachInit'] = np.nan\n",
    "        new_rows['reachMax'] = np.nan\n",
    "        new_rows['reachEnd'] = np.nan\n",
    "        new_rows['stim'] = 0\n",
    "        new_rows['behaviors'] = np.nan\n",
    "        new_rows['ReachType'] = np.nan\n",
    "\n",
    "        # # Reorder columns to match original file\n",
    "        # new_rows = new_rows[['reachinit', 'reachmax', 'reachend', 'stim',\n",
    "        #                      'pellet_delivery', 'pellet_detected', 't6000', 't5000', 'behaviors']]\n",
    "\n",
    "        # Reorder columns to match original file\n",
    "        new_rows = new_rows[['T6000', 'T5000', 'reachInit', 'reachMax',\n",
    "                            'reachEnd', 'stim','behaviors' ,'pellet_delivery', 'pellet_detected','ReachType']]\n",
    "\n",
    "        # Combine and sort by T6000\n",
    "        combined = pd.concat([df_orig, new_rows], ignore_index=True)\n",
    "        combined = combined.sort_values(by='T6000').reset_index(drop=True)\n",
    "\n",
    "        # Force final column order\n",
    "        combined = combined[['T6000', 'T5000', 'reachInit', 'reachMax',\n",
    "                            'reachEnd', 'stim', 'behaviors',\n",
    "                            'pellet_delivery', 'pellet_detected','ReachType']]\n",
    "\n",
    "\n",
    "        # ---- Step 2: Save output as the same original file (overwrite it)\n",
    "        xlsx_save_path = xlsx_file\n",
    "        self.xlsx_file = xlsx_save_path\n",
    "\n",
    "        original_dir = os.path.dirname(self.xlsx_file)\n",
    "        backup_dir = os.path.join(original_dir,'xlsx_backups',session_save_name ,'reach_curator_direct_backup')\n",
    "        backup_file_path = os.path.join(backup_dir, os.path.basename(self.xlsx_file))\n",
    "\n",
    "        backup_added_rows = os.path.join(original_dir, 'xlsx_backups' ,session_save_name,'added_missing_rows_backup')\n",
    "        os.makedirs(backup_added_rows, exist_ok=True)\n",
    "        backup_added_rows_file_path = os.path.join(backup_added_rows, os.path.basename(self.xlsx_file))\n",
    "\n",
    "        # Save the combined DataFrame to the same xlsx file\n",
    "        if save_df:\n",
    "            if os.path.exists(backup_file_path):\n",
    "                combined.to_excel(xlsx_save_path, index=False)\n",
    "                combined.to_excel(backup_added_rows_file_path, index=False)\n",
    "                print(f\"✅ Final merged Excel saved to: {xlsx_save_path}\")\n",
    "                print(f\"✅ Backup saved to: {backup_added_rows_file_path}\")\n",
    "                return combined, xlsx_save_path\n",
    "            else:\n",
    "                print(f\"⚠️Warning: Backup file doesnt exists⚠️\")\n",
    "                print('❌ Stopping the script to avoid overwriting the xlsx file without a back up❌')\n",
    "                print(f\"⚠️Warning: {backup_file_path} does not exist⚠️\")\n",
    "                return None, None\n",
    "        else:\n",
    "            print(f\"⚠️Warning: DataFrame not saved to {xlsx_save_path}⚠️\")\n",
    "            print('set save_df to True to save the DataFrame')\n",
    "            return combined\n",
    "    \n",
    "    def create_new_xlxs_file(self):\n",
    "        save_df = self.save_df\n",
    "        txt_file = self.txt_file\n",
    "        xlsx_file = self.xlsx_file\n",
    "\n",
    "        # 1) Validate TXT and XLSX existence\n",
    "        if not os.path.exists(txt_file):\n",
    "            print(f\"⚠️ Warning: {txt_file} does not exist\")\n",
    "            return None, None\n",
    "        if not os.path.exists(xlsx_file):\n",
    "            print(f\"⚠️ Warning: {xlsx_file} does not exist\")\n",
    "            return None, None\n",
    "\n",
    "        # 2) Load original Excel into DataFrame\n",
    "        df_orig = pd.read_excel(xlsx_file)\n",
    "        if df_orig.empty:\n",
    "            print(\"⚠️ Warning: The xlsx file is empty\")\n",
    "            return None, None\n",
    "        df_orig.columns = df_orig.columns.str.strip()\n",
    "        # Mark all original rows\n",
    "        df_orig['Added_row'] = ''\n",
    "\n",
    "        # 3) Parse event TXT file\n",
    "        events = []\n",
    "        current_event = {}\n",
    "        with open(txt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    key, val = line.strip().split('\\t')\n",
    "                    value = int(val)\n",
    "                    if key == 'T6000_played':\n",
    "                        if current_event:\n",
    "                            events.append(current_event)\n",
    "                        current_event = {'T6000': value}\n",
    "                    elif key == 'pellet_detected':\n",
    "                        current_event['pellet_detected'] = value\n",
    "                    elif key == 'pellet_delivery':\n",
    "                        current_event['pellet_delivery'] = value\n",
    "                    elif key == 'T5000_played':\n",
    "                        current_event['T5000'] = value\n",
    "        if current_event:\n",
    "            events.append(current_event)\n",
    "        events_df = pd.DataFrame(events)\n",
    "\n",
    "        # 4) Identify and build new rows\n",
    "        existing_T6000s = df_orig['T6000'].values\n",
    "        new_rows = events_df[~events_df['T6000'].isin(existing_T6000s)].copy()\n",
    "        # Initialize required columns\n",
    "        for col in ['reachInit','reachMax','reachEnd','behaviors','Reach_type']:\n",
    "            new_rows[col] = np.nan if 'reach' in col or col in ['behaviors'] else new_rows.get(col, np.nan)\n",
    "        new_rows['stim'] = 0\n",
    "        # Flag added rows with T5000\n",
    "        new_rows['Added_row'] = new_rows['T5000']\n",
    "\n",
    "        # 5) Enforce final column order\n",
    "        cols = [\n",
    "            'T6000','T5000','Added_row',\n",
    "            'reachInit','reachMax','reachEnd',\n",
    "            'stim','behaviors','pellet_delivery',\n",
    "            'pellet_detected','Reach_type'\n",
    "        ]\n",
    "        df_orig = df_orig[cols]\n",
    "        new_rows = new_rows[cols]\n",
    "\n",
    "        # 6) Combine, sort, and reset index\n",
    "        combined = pd.concat([df_orig, new_rows], ignore_index=True)\n",
    "        combined = combined.sort_values('T6000', ignore_index=True)\n",
    "\n",
    "        # 7) Backup paths\n",
    "        base_dir = os.path.dirname(xlsx_file)\n",
    "        primary_backup_dir = os.path.join(base_dir, 'xlsx_backups', session_save_name, 'reach_curator_direct_backup')\n",
    "        primary_backup_file = os.path.join(primary_backup_dir, os.path.basename(xlsx_file))\n",
    "        added_backup_dir = os.path.join(base_dir, 'xlsx_backups', session_save_name, 'added_missing_rows_backup')\n",
    "        os.makedirs(added_backup_dir, exist_ok=True)\n",
    "        added_backup_file = os.path.join(added_backup_dir, os.path.basename(xlsx_file))\n",
    "\n",
    "        # 8) Save combined DataFrame and backups\n",
    "        if save_df:\n",
    "            if os.path.exists(primary_backup_file):\n",
    "                combined.to_excel(xlsx_file, index=False)\n",
    "                combined.to_excel(added_backup_file, index=False)\n",
    "                print(f\"✅ Final merged Excel saved to: {xlsx_file}\")\n",
    "                print(f\"✅ Added-rows backup saved to: {added_backup_file}\")\n",
    "                return combined, xlsx_file\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: Primary backup missing at {primary_backup_file}\")\n",
    "                print(\"❌ Aborting: primary backup required before overwrite\")\n",
    "                return None, None\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: save_df is False—DataFrame created but not saved to {xlsx_file}\")\n",
    "            return combined, None\n",
    "\n",
    "    def drop_empty_reach_rows(self):\n",
    "      \n",
    "        df = pd.read_excel(self.xlsx_file)\n",
    "        # back up the newly created xlsx file\n",
    "        backup_dir_02 = os.path.join(os.path.dirname(self.xlsx_file),'xlsx_backups', session_save_name,'completed_manual_curation_backup')\n",
    "        os.makedirs(backup_dir_02, exist_ok=True)\n",
    "        backup_file_path_02 = os.path.join(backup_dir_02, os.path.basename(self.xlsx_file))\n",
    "        if os.path.exists(backup_file_path_02):\n",
    "            print(f\"⚠️Warning: Backup file already exists at {backup_file_path_02}⚠️\")\n",
    "            print('❌ Stopping the script to avoid overwriting the backup file ❌')\n",
    "            return None \n",
    "        else:\n",
    "            # Copy the original file to the backup directory\n",
    "            shutil.copy2(self.xlsx_file, backup_file_path_02)\n",
    "            print(f\"✅ Backup saved to: {backup_file_path_02}\")\n",
    "        \"\"\"\n",
    "        Drop specified rows from the DataFrame.\n",
    "        \"\"\"\n",
    "        # Drop bad rows\n",
    "        df = df.dropna(subset=['reachInit']).reset_index(drop=True)\n",
    "         # save the DataFrame to the same xlsx file\n",
    "        if self.save_df:\n",
    "            df.to_excel(self.xlsx_file, index=False)\n",
    "            print(f\"✅ Final merged Excel saved to: {self.xlsx_file}\")   \n",
    "              \n",
    "        return df\n",
    "    \n",
    "    def load_orig_xlsx_file(self):\n",
    "        \"\"\"\n",
    "        Load the original xlsx file.\n",
    "        \"\"\"\n",
    "        original_dir = os.path.dirname(self.xlsx_file)\n",
    "        backup_dir = os.path.join(original_dir, 'xlsx_backups',session_save_name ,'reach_curator_direct_backup')\n",
    "        backup_file_path = os.path.join(backup_dir, os.path.basename(self.xlsx_file))\n",
    "\n",
    "        df_orig = pd.read_excel(backup_file_path)\n",
    "        return df_orig, backup_file_path\n",
    "    \n",
    "    def load_added_rows_xlsx(self):\n",
    "        \"\"\"\n",
    "        Load the added rows xlsx file.\n",
    "        \"\"\"\n",
    "        original_dir = os.path.dirname(self.xlsx_file)\n",
    "        backup_added_rows = os.path.join(original_dir,'xlsx_backups' ,session_save_name, 'added_missing_rows_backup')\n",
    "        backup_added_rows_file_path = os.path.join(backup_added_rows, os.path.basename(self.xlsx_file))\n",
    "\n",
    "        df_added_rows = pd.read_excel(backup_added_rows_file_path)\n",
    "        return df_added_rows, backup_added_rows_file_path\n",
    "    \n",
    "    \n",
    "    def load_final_xlsx(self):\n",
    "        \"\"\"\n",
    "        Load the combined xlsx file.\n",
    "        \"\"\"\n",
    "        df_final = pd.read_excel(self.xlsx_file)\n",
    "        return df_final, self.xlsx_file\n",
    "    \n",
    "\n",
    "    def add_optical_stim(self, frame_tol=1125):\n",
    "        \"\"\"\n",
    "        Compare the master .xlsx’s T5000 column to df_closedLoop_stim_loaded’s\n",
    "        T5000_frame, and mark stim_present=True whenever a match falls within ±tol.\n",
    "        \"\"\"\n",
    "        # 1) load master sheet\n",
    "        main_df = pd.read_excel(self.xlsx_file)\n",
    "\n",
    "        # 2) make sure your closed-loop-stim DataFrame is available\n",
    "        #    here I assume df_closedLoop_stim_loaded is in scope already\n",
    "        closed_df = df_closedLoop_stim_loaded[['T5000_frame', 'stim_present']].copy()\n",
    "\n",
    "        # 3) sort both by the key columns (required by merge_asof)\n",
    "        main_df   = main_df.sort_values('T5000').reset_index(drop=True)\n",
    "        closed_df = closed_df.sort_values('T5000_frame').reset_index(drop=True)\n",
    "\n",
    "        # 4) nearest‐join within tolerance\n",
    "        merged = pd.merge_asof(\n",
    "            main_df,\n",
    "            closed_df,\n",
    "            left_on='T5000',\n",
    "            right_on='T5000_frame',\n",
    "            tolerance=frame_tol,\n",
    "            direction='nearest'\n",
    "        )\n",
    "\n",
    "        # 5) drop the helper column and return\n",
    "        merged = merged.drop(columns='T5000_frame')\n",
    "\n",
    "\n",
    "        # back up the most recent xlsx file\n",
    "        backup_dir = os.path.join(os.path.dirname(self.xlsx_file),'xlsx_backups', session_save_name,'closedLoop_stim_backup')\n",
    "        os.makedirs(backup_dir, exist_ok=True)\n",
    "        backup_file_path = os.path.join(backup_dir, os.path.basename(self.xlsx_file))\n",
    "        if os.path.exists(backup_file_path):\n",
    "            print(f\"⚠️Warning: Backup file already exists at {backup_file_path}⚠️\")\n",
    "            print('❌ Stopping the script to avoid overwriting the backup file ❌')\n",
    "            return None \n",
    "        else:\n",
    "            # Copy the current original file to the backup directory\n",
    "            shutil.copy2(self.xlsx_file, backup_file_path)\n",
    "            print(f\"✅ Backup saved to: {backup_file_path}\")\n",
    "\n",
    "         # save the DataFrame to the main xlsx file location\n",
    "        merged.to_excel(self.xlsx_file, index=False)\n",
    "        print(f\"✅ Final closed loop optical stim df saved to: {self.xlsx_file}\")   \n",
    "              \n",
    "        return merged\n",
    "\n",
    "    \n",
    "    def add_optical_stim_to_raw_added_rows_df(self, frame_tol=1125):\n",
    "            \"\"\"\n",
    "            Compare the master .xlsx’s T5000 column to df_closedLoop_stim_loaded\n",
    "            T5000_frame, and mark stim_present=True whenever a match falls within ±tol.\n",
    "            \"\"\"\n",
    "            # 1) load master sheet\n",
    "            main_df = pd.read_excel(self.xlsx_file)\n",
    "\n",
    "            original_dir = os.path.dirname(self.xlsx_file)\n",
    "            backup_added_rows = os.path.join(original_dir,'xlsx_backups' ,session_save_name, 'added_missing_rows_backup')\n",
    "            backup_added_rows_file_path = os.path.join(backup_added_rows, os.path.basename(self.xlsx_file))\n",
    "\n",
    "            df_added_rows = pd.read_excel(backup_added_rows_file_path)\n",
    "\n",
    "\n",
    "            # 2) make sure your closed-loop-stim DataFrame is available\n",
    "            #    here I assume df_closedLoop_stim_loaded is in scope already\n",
    "            closed_df = df_closedLoop_stim_loaded[['T5000_frame', 'stim_present']].copy()\n",
    "\n",
    "            # 3) sort both by the key columns (required by merge_asof)\n",
    "            df_added_rows   = df_added_rows.sort_values('T5000').reset_index(drop=True)\n",
    "            closed_df = closed_df.sort_values('T5000_frame').reset_index(drop=True)\n",
    "\n",
    "            # 4) nearest‐join within tolerance\n",
    "            merged = pd.merge_asof(\n",
    "                df_added_rows,\n",
    "                closed_df,\n",
    "                left_on='T5000',\n",
    "                right_on='T5000_frame',\n",
    "                tolerance=frame_tol,\n",
    "                direction='nearest'\n",
    "            )\n",
    "\n",
    "            # 5) drop the helper column and return\n",
    "            merged = merged.drop(columns='T5000_frame')\n",
    "\n",
    "\n",
    "            # 6) saved the merged \n",
    "            closedLoop_stim_root_dir = os.path.join(os.path.dirname(self.xlsx_file),'xlsx_backups', session_save_name,'closedLoop_stim')\n",
    "            closedLoop_stim_on_added_rows_root_dir = os.path.join(closedLoop_stim_root_dir,'raw_added_rows_version')\n",
    "            closedLoop_stim_on_added_rows_dir = os.path.join(closedLoop_stim_on_added_rows_root_dir ,os.path.basename(self.xlsx_file))\n",
    "            if os.path.exists(closedLoop_stim_root_dir):\n",
    "                print(f'closedLoop_stim_dir Exists Already --> {closedLoop_stim_root_dir}')\n",
    "            else:\n",
    "                os.makedirs(closedLoop_stim_root_dir, exist_ok=True)\n",
    "                print(f'Created closedLoop_stim_dir --> {closedLoop_stim_root_dir}')\n",
    "\n",
    "            if os.path.exists(closedLoop_stim_on_added_rows_root_dir):\n",
    "                print(f'Skipping Save to stop overwrite: because closedLoop_stim_df already exists --> {closedLoop_stim_on_added_rows_root_dir} ')\n",
    "            else:\n",
    "                os.makedirs(closedLoop_stim_on_added_rows_root_dir, exist_ok=True)\n",
    "                merged.to_excel(closedLoop_stim_on_added_rows_dir,index=False)\n",
    "                print(f'Saved out closed loop optical stim df --> {closedLoop_stim_on_added_rows_root_dir}')\n",
    "\n",
    "            return merged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_fail_modify = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final merged Excel saved to: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\20241114_christielab_session010.xlsx\n",
      "✅ Added-rows backup saved to: G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\reach3_20241114_session010\\added_missing_rows_backup\\20241114_christielab_session010.xlsx\n"
     ]
    }
   ],
   "source": [
    "if safe_fail_modify:\n",
    "    mouse_sessions = modify_curator_xlsx_file(xlsx_file, txt_file,save_df=True)\n",
    "    df_modied, xlsx_saved_file = mouse_sessions.create_new_xlxs_file()\n",
    "    df_orig, xlsx_orig_file = mouse_sessions.load_orig_xlsx_file()\n",
    "    df_added_rows, xlsx_added_rows_file = mouse_sessions.load_added_rows_xlsx()\n",
    "    df_final, xlsx_final_file = mouse_sessions.load_final_xlsx()\n",
    "    df_modied\n",
    "else:\n",
    "    print(f\"⚠️Warning: safe_fail actviated⚠️\")\n",
    "    print('if you want to run the code, please re-run this cell')\n",
    "    print('Note this ')\n",
    "    safe_fail_modify = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in closed loop optical stimulation\n",
    "- adds the stim_present coulmn to the .xlsx file\n",
    "- puts a 1 if there was a closed loop optical stim that trial \n",
    "#### Bonus you can run this at any point\n",
    "- meaning if you already started adding reaches with the curator it will work, because it simply matches the frame numbers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_fail_modify = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️Warning: Backup file already exists at G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\reach3_20241114_session010\\closedLoop_stim_backup\\20241114_christielab_session010.xlsx⚠️\n",
      "❌ Stopping the script to avoid overwriting the backup file ❌\n"
     ]
    }
   ],
   "source": [
    "if safe_fail_modify:\n",
    "    # View current main .xlsx file\n",
    "    mouse_sessions = modify_curator_xlsx_file(xlsx_file, txt_file,save_df=True)\n",
    "    closedLoop_df = mouse_sessions.add_optical_stim()\n",
    "    closedLoop_df\n",
    "else:\n",
    "    print(f\"⚠️Warning: safe_fail actviated⚠️\")\n",
    "    print(' This will add in the pellet_present coulmn to the df --> this only works if you already created the df_closedLoop_stim_loaded df when making the NWB with analyze_data/01_event_plots_makeNWB.ipynb')\n",
    "    print('if you want to run the code, please re-run this cell')\n",
    "    print('Note this ')\n",
    "    safe_fail_modify = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xlsx_saved_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load existing curated xlsx\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_added_missing_rows \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[43mxlsx_saved_file\u001b[49m)\n\u001b[0;32m      3\u001b[0m df_added_missing_rows\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xlsx_saved_file' is not defined"
     ]
    }
   ],
   "source": [
    "# Load existing curated xlsx\n",
    "df_added_missing_rows = pd.read_excel(xlsx_saved_file)\n",
    "df_added_missing_rows.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig, backup_file_path = mouse_sessions.load_orig_xlsx_file()\n",
    "df_added_missing_rows, backup_added_rows_file_path = mouse_sessions.load_added_rows_xlsx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "loading orginal_df for reach events from --> G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\reach3_20241114_session010\\reach_curator_direct_backup\\20241114_christielab_session010.xlsx\n",
      "loading df with missing rows added from--> G:\\Grant\\behavior_data\\DLC_net\\grant_reach3_swingDoor-christie\\Grant_curate\\xlsx_backups\\reach3_20241114_session010\\added_missing_rows_backup\\20241114_christielab_session010.xlsx\n",
      "\n",
      "Original reach events total count: 7\n",
      "Added reach events total count: 419\n",
      "\n",
      "❌ Reaches Missed: 412\n"
     ]
    }
   ],
   "source": [
    "orig_reach_count = df_orig.shape[0]\n",
    "added_row_count = df_added_missing_rows.shape[0]\n",
    "print('------')\n",
    "print(f'loading orginal_df for reach events from --> {backup_file_path}')\n",
    "print(f'loading df with missing rows added from--> {backup_added_rows_file_path}')\n",
    "# Count the number of reach events\n",
    "print('')\n",
    "print(f\"Original reach events total count: {len(df_orig)}\")\n",
    "print(f\"Added reach events total count: {len(df_added_missing_rows)}\")\n",
    "print('')\n",
    "print(f'❌ Reaches Missed: {added_row_count- orig_reach_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reachInit</th>\n",
       "      <th>reachMax</th>\n",
       "      <th>reachEnd</th>\n",
       "      <th>stim</th>\n",
       "      <th>pellet_delivery</th>\n",
       "      <th>pellet_detected</th>\n",
       "      <th>T6000</th>\n",
       "      <th>T5000</th>\n",
       "      <th>behaviors</th>\n",
       "      <th>Reach_type</th>\n",
       "      <th>Added_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86819</td>\n",
       "      <td>86835</td>\n",
       "      <td>86846</td>\n",
       "      <td>0</td>\n",
       "      <td>85455</td>\n",
       "      <td>85529</td>\n",
       "      <td>85374</td>\n",
       "      <td>85457</td>\n",
       "      <td>grabbed</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287050</td>\n",
       "      <td>287067</td>\n",
       "      <td>287082</td>\n",
       "      <td>0</td>\n",
       "      <td>286360</td>\n",
       "      <td>270230</td>\n",
       "      <td>286199</td>\n",
       "      <td>286369</td>\n",
       "      <td>dropped</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>328788</td>\n",
       "      <td>328805</td>\n",
       "      <td>328814</td>\n",
       "      <td>0</td>\n",
       "      <td>327072</td>\n",
       "      <td>270230</td>\n",
       "      <td>326936</td>\n",
       "      <td>327080</td>\n",
       "      <td>dropped</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>342256</td>\n",
       "      <td>342270</td>\n",
       "      <td>342275</td>\n",
       "      <td>0</td>\n",
       "      <td>340424</td>\n",
       "      <td>270230</td>\n",
       "      <td>340344</td>\n",
       "      <td>340424</td>\n",
       "      <td>dropped</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>475419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>473492</td>\n",
       "      <td>473570</td>\n",
       "      <td>473388</td>\n",
       "      <td>473495</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>538682</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>534860</td>\n",
       "      <td>534934</td>\n",
       "      <td>538642</td>\n",
       "      <td>534862</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1004054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002078</td>\n",
       "      <td>1002153</td>\n",
       "      <td>1001998</td>\n",
       "      <td>1002080</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reachInit  reachMax  reachEnd  stim  pellet_delivery  pellet_detected  \\\n",
       "0      86819     86835     86846     0            85455            85529   \n",
       "1     287050    287067    287082     0           286360           270230   \n",
       "2     328788    328805    328814     0           327072           270230   \n",
       "3     342256    342270    342275     0           340424           270230   \n",
       "4     475419         0         0     0           473492           473570   \n",
       "5     538682         0         0     0           534860           534934   \n",
       "6    1004054         0         0     0          1002078          1002153   \n",
       "\n",
       "     T6000    T5000 behaviors  Reach_type  Added_row  \n",
       "0    85374    85457   grabbed           0        NaN  \n",
       "1   286199   286369   dropped           0        NaN  \n",
       "2   326936   327080   dropped           0        NaN  \n",
       "3   340344   340424   dropped           0        NaN  \n",
       "4   473388   473495      none           0        NaN  \n",
       "5   538642   534862      none           0        NaN  \n",
       "6  1001998  1002080      none           0        NaN  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T6000</th>\n",
       "      <th>T5000</th>\n",
       "      <th>Added_row</th>\n",
       "      <th>reachInit</th>\n",
       "      <th>reachMax</th>\n",
       "      <th>reachEnd</th>\n",
       "      <th>stim</th>\n",
       "      <th>behaviors</th>\n",
       "      <th>pellet_delivery</th>\n",
       "      <th>pellet_detected</th>\n",
       "      <th>Reach_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1744</td>\n",
       "      <td>1829</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1826</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3843</td>\n",
       "      <td>4002</td>\n",
       "      <td>4002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3999</td>\n",
       "      <td>4076.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6019</td>\n",
       "      <td>6103</td>\n",
       "      <td>6103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6100</td>\n",
       "      <td>6177.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8081</td>\n",
       "      <td>8165</td>\n",
       "      <td>8165.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8162</td>\n",
       "      <td>8237.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10519</td>\n",
       "      <td>10602</td>\n",
       "      <td>10602.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10600</td>\n",
       "      <td>10676.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13143</td>\n",
       "      <td>13228</td>\n",
       "      <td>13228.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13226</td>\n",
       "      <td>13301.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15845</td>\n",
       "      <td>15926</td>\n",
       "      <td>15926.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15924</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18168</td>\n",
       "      <td>18252</td>\n",
       "      <td>18252.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18250</td>\n",
       "      <td>18325.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20645</td>\n",
       "      <td>20728</td>\n",
       "      <td>20728.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20726</td>\n",
       "      <td>20800.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22934</td>\n",
       "      <td>23093</td>\n",
       "      <td>23093.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23091</td>\n",
       "      <td>23165.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25110</td>\n",
       "      <td>25192</td>\n",
       "      <td>25192.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25189</td>\n",
       "      <td>25340.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27208</td>\n",
       "      <td>27291</td>\n",
       "      <td>27291.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27289</td>\n",
       "      <td>27365.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29271</td>\n",
       "      <td>29355</td>\n",
       "      <td>29355.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29353</td>\n",
       "      <td>29428.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31484</td>\n",
       "      <td>31568</td>\n",
       "      <td>31568.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31565</td>\n",
       "      <td>31641.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34073</td>\n",
       "      <td>34230</td>\n",
       "      <td>34230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34228</td>\n",
       "      <td>34303.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    T6000  T5000  Added_row  reachInit  reachMax  reachEnd  stim behaviors  \\\n",
       "0    1744   1829     1829.0        NaN       NaN       NaN     0       NaN   \n",
       "1    3843   4002     4002.0        NaN       NaN       NaN     0       NaN   \n",
       "2    6019   6103     6103.0        NaN       NaN       NaN     0       NaN   \n",
       "3    8081   8165     8165.0        NaN       NaN       NaN     0       NaN   \n",
       "4   10519  10602    10602.0        NaN       NaN       NaN     0       NaN   \n",
       "5   13143  13228    13228.0        NaN       NaN       NaN     0       NaN   \n",
       "6   15845  15926    15926.0        NaN       NaN       NaN     0       NaN   \n",
       "7   18168  18252    18252.0        NaN       NaN       NaN     0       NaN   \n",
       "8   20645  20728    20728.0        NaN       NaN       NaN     0       NaN   \n",
       "9   22934  23093    23093.0        NaN       NaN       NaN     0       NaN   \n",
       "10  25110  25192    25192.0        NaN       NaN       NaN     0       NaN   \n",
       "11  27208  27291    27291.0        NaN       NaN       NaN     0       NaN   \n",
       "12  29271  29355    29355.0        NaN       NaN       NaN     0       NaN   \n",
       "13  31484  31568    31568.0        NaN       NaN       NaN     0       NaN   \n",
       "14  34073  34230    34230.0        NaN       NaN       NaN     0       NaN   \n",
       "\n",
       "    pellet_delivery  pellet_detected  Reach_type  \n",
       "0              1826           1900.0         NaN  \n",
       "1              3999           4076.0         NaN  \n",
       "2              6100           6177.0         NaN  \n",
       "3              8162           8237.0         NaN  \n",
       "4             10600          10676.0         NaN  \n",
       "5             13226          13301.0         NaN  \n",
       "6             15924          16000.0         NaN  \n",
       "7             18250          18325.0         NaN  \n",
       "8             20726          20800.0         NaN  \n",
       "9             23091          23165.0         NaN  \n",
       "10            25189          25340.0         NaN  \n",
       "11            27289          27365.0         NaN  \n",
       "12            29353          29428.0         NaN  \n",
       "13            31565          31641.0         NaN  \n",
       "14            34228          34303.0         NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_added_missing_rows.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❌ END ❌\n",
    "\n",
    "\n",
    "### ✅ Next Steps\n",
    "1. Go back to reach_Curator_p38_v3.py, and simply launch it liuke normal. then manually place all the reaches you need \n",
    "\n",
    "2. Once 100% done adding reachs and label. open the analyze_curated_reach_results.ipynb file (its in the same folder as this notebook)\n",
    "    - then simply run that file\n",
    "    - it will first back up your manaual curation .xlsx file too --> Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/completed_manual_curation_backup\n",
    "    - it will extract / drop all empty rows from that .xlsx file and save the final version too -->  Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/final_backup\n",
    "    - and it will save this final_df to the root folder, so the reach_curator can load it in --> Grant_curate/final_xlsx_file.xlsx \n",
    "\n",
    "\n",
    "# ✅ Results of file\n",
    "1. you have created a duplicate of the .xlsx file that the reach_curator_py38_v3.py makes when you load in a session for the first time \n",
    "    - Backup location --> Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/reach_curator_direct_backup\n",
    "    - Note, this file is missing all the T6000 and T5000 rows that the find_reach_events.ipynb did not associate a reach too\n",
    "2. you have created a new .xlsx file that now containes every sinlge row, meaning it added rows for every single T6000 > T5000, and left the reachInit > reachMax > reachEnd vlaues empty, this way you can manually go into the curator and add reaches for these rows\n",
    "    - Backup location --> Grant_curate/xlsx_backups/mouse_sessionDate_sessionID/added_missing_rows_backup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reaching_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
